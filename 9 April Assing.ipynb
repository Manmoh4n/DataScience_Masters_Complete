{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73c2b93-8f6c-4c42-997e-c7f0005bc7e0",
   "metadata": {},
   "source": [
    "# Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed7d7f-fe28-417a-b83c-0761e749ece9",
   "metadata": {},
   "source": [
    "Bayes' theorem allows us to calculate the probability of a hypothesis being true, given some observed evidence. It provides a way to update our beliefs about the likelihood of an event or hypothesis based on new information.\n",
    "The theorem is particularly useful in situations where we have prior knowledge about the probability of an event, and we want to incorporate new evidence to update our beliefs. It is widely used in fields such as statistics, machine learning, and data science for tasks such as Bayesian inference, classification, and hypothesis testing.\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(A|B) represents the conditional probability of event A given that event B has occurred.\n",
    "P(B|A) represents the conditional probability of event B given that event A has occurred.\n",
    "P(A) and P(B) represent the probabilities of events A and B, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3620f878-e402-4e6c-8db5-b475efceec3a",
   "metadata": {},
   "source": [
    "# Q2. What is the formula for Bayes' theorem?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c9f1b-6c2d-4c7d-8f60-c4d72578ef7a",
   "metadata": {},
   "source": [
    "The formula for Bayes' theorem is as follows:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(A|B) represents the conditional probability of event A given that event B has occurred.\n",
    "P(B|A) represents the conditional probability of event B given that event A has occurred.\n",
    "P(A) and P(B) represent the probabilities of events A and B, respectively.\n",
    "To understand the formula, it's helpful to break it down:\n",
    "\n",
    "P(A|B): This represents the probability of event A occurring given that event B has already happened. It's the probability you're trying to calculate or update based on new evidence.\n",
    "\n",
    "P(B|A): This represents the probability of event B occurring given that event A has already happened. It quantifies the likelihood of observing the evidence (B) if the hypothesis (A) is true.\n",
    "\n",
    "P(A): This is the probability of event A occurring independently of any evidence or new information. It represents your initial belief or prior probability about event A.\n",
    "\n",
    "P(B): This is the probability of event B occurring independently of any evidence or new information. It represents the overall likelihood of observing event B.\n",
    "\n",
    "By multiplying P(B|A) and P(A), you calculate the joint probability of both A and B happening together. Then, dividing this joint probability by P(B) normalizes the result and gives you the updated probability of A given B.\n",
    "\n",
    "Bayes' theorem is a powerful tool for updating probabilities and making informed decisions based on new evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f69a8f5-57db-4309-b910-73d262f08f47",
   "metadata": {},
   "source": [
    "# Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2961c-0d34-4a09-992f-3665680d4a72",
   "metadata": {},
   "source": [
    "Bayes' theorem is used in a variety of practical applications across different fields. Here are a few common use cases:\n",
    "\n",
    "Bayesian Inference: Bayes' theorem is fundamental to Bayesian inference, a statistical approach used to update probability distributions based on observed data. It allows for the incorporation of prior knowledge and new evidence to make more accurate predictions and estimates.\n",
    "\n",
    "Medical Diagnosis: Bayes' theorem is employed in medical diagnosis. By considering prior probabilities of diseases, the likelihood of specific symptoms occurring given those diseases, and the overall probability of observing the symptoms, doctors can update their diagnosis and assess the probability of a patient having a particular condition.\n",
    "\n",
    "Spam Filtering: Many email spam filters utilize Bayes' theorem to classify incoming emails as spam or legitimate. The filter calculates the probabilities of certain words or patterns occurring in spam or non-spam messages, and then updates the probabilities based on the presence or absence of those words in an incoming email to make a classification decision.\n",
    "\n",
    "Machine Learning and AI: Bayes' theorem serves as the foundation for various machine learning algorithms. Naive Bayes classifiers, for instance, use the theorem to estimate the probability of a particular class given observed features. This enables classification tasks in areas like text categorization, sentiment analysis, and recommendation systems.\n",
    "\n",
    "Risk Assessment: Bayes' theorem can be employed in risk assessment and decision-making. By incorporating prior probabilities and new evidence, it allows for more informed judgments about the likelihood of certain events or outcomes and aids in managing and mitigating risks.\n",
    "\n",
    "Quality Control: Bayes' theorem is useful in quality control processes. It helps determine the likelihood of a defective item given certain test results, allowing for adjustments in production processes or decisions about accepting or rejecting batches based on the probability calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0b3579-92f0-4f45-a546-eca5ef9aef36",
   "metadata": {},
   "source": [
    "# Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b1a1d1-1b20-49c5-af07-668d9e42d083",
   "metadata": {},
   "source": [
    "Bayes' theorem is closely related to conditional probability. In fact, Bayes' theorem can be derived from the principles of conditional probability.\n",
    "\n",
    "Conditional probability is the probability of an event occurring given that another event has already occurred. It is denoted as P(A|B), where A and B are events.\n",
    "\n",
    "Bayes' theorem provides a way to calculate the conditional probability P(A|B) using the reverse conditional probability P(B|A) and the probabilities of the individual events A and B.\n",
    "\n",
    "The relationship between Bayes' theorem and conditional probability can be seen in the formula:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "Here, P(A|B) is the conditional probability of event A given event B, and P(B|A) is the conditional probability of event B given event A. P(A) and P(B) are the probabilities of events A and B, respectively.\n",
    "\n",
    "Bayes' theorem allows us to update our beliefs about the probability of event A given event B by considering the likelihood of event B occurring given event A, as well as the probabilities of events A and B independently.\n",
    "\n",
    "In summary, Bayes' theorem provides a way to express the conditional probability of an event in terms of other conditional probabilities and individual event probabilities. It is a powerful tool for updating probabilities based on new evidence and incorporating prior knowledge.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa6f4d-189f-44b8-a86b-62d7cf5a56cf",
   "metadata": {},
   "source": [
    "# Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43e2c0-7f1c-43c8-924a-64460e2d6868",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes:\n",
    "\n",
    "Suitable for continuous data where the features follow a Gaussian (normal) distribution.\n",
    "Assumes that the features are independent and have equal variance within each class.\n",
    "Appropriate when the continuous features can be reasonably approximated by a bell curve.\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Suitable for discrete features, such as word counts or occurrence frequencies.\n",
    "Assumes that the features are independent and follow a multinomial distribution.\n",
    "Commonly used for text classification tasks like sentiment analysis or spam filtering.\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Suitable for binary features, where each feature represents the presence or absence of a particular attribute.\n",
    "Assumes that features are independent and follow a Bernoulli distribution.\n",
    "Typically used for document classification tasks or problems with binary features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d68d2c-9adf-42f5-8fc1-74b4f6b71908",
   "metadata": {},
   "source": [
    "# Q6. Assignment:\n",
    "# You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "# Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "# each feature value for each class:\n",
    "# Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "# A 3 3 4 4 3 3 3\n",
    "# B 2 2 1 2 2 2 3\n",
    "# Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instanceto belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e854ec2b-b731-4a59-a86f-363cb0908cf3",
   "metadata": {},
   "source": [
    "Given the frequency table, we can calculate the conditional probabilities as follows:\n",
    "\n",
    "P(X1=3|A) = 4/13\n",
    "P(X2=4|A) = 3/13\n",
    "\n",
    "P(X1=3|B) = 1/10\n",
    "P(X2=4|B) = 3/10\n",
    "\n",
    "Since we assume equal prior probabilities for each class, P(A) = P(B) = 0.5.\n",
    "\n",
    "Now, let's calculate the posterior probabilities for each class using Bayes' theorem:\n",
    "\n",
    "P(A|X1=3, X2=4) = (P(X1=3|A) * P(X2=4|A) * P(A)) / (P(X1=3) * P(X2=4))\n",
    "= (4/13 * 3/13 * 0.5) / (P(X1=3) * P(X2=4))\n",
    "\n",
    "P(B|X1=3, X2=4) = (P(X1=3|B) * P(X2=4|B) * P(B)) / (P(X1=3) * P(X2=4))\n",
    "= (1/10 * 3/10 * 0.5) / (P(X1=3) * P(X2=4))\n",
    "\n",
    "Since the prior probabilities P(A) and P(B) are equal and the denominator is the same for both classes, we can compare the numerators:\n",
    "\n",
    "P(A|X1=3, X2=4) = (4/13 * 3/13 * 0.5)\n",
    "P(B|X1=3, X2=4) = (1/10 * 3/10 * 0.5)\n",
    "\n",
    "Calculating the values:\n",
    "\n",
    "P(A|X1=3, X2=4) ≈ 0.043\n",
    "P(B|X1=3, X2=4) ≈ 0.015\n",
    "\n",
    "Based on these calculations, Naive Bayes predicts that the new instance with features X1=3 and X2=4 is more likely to belong to class A, as it has a higher posterior probability compared to class B.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
