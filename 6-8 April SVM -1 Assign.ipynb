{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1591a35-85f3-4060-92a0-eadb30de8596",
   "metadata": {},
   "source": [
    "# Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2730678e-8485-4155-b80d-c2ba0e930459",
   "metadata": {},
   "source": [
    "A Support Vector Machine (SVM) is a machine learning algorithm that can be used for both classification and regression analysis. In the case of a linear SVM, the mathematical formula for the decision boundary is given by:\n",
    "\n",
    "w^T x + b = 0\n",
    "\n",
    "where w is a vector of weights, x is a vector of input features, b is a bias term, and the superscript T denotes the transpose operation.\n",
    "\n",
    "To classify a new input vector, the SVM calculates the signed distance of the input vector to the decision boundary, which is given by:\n",
    "\n",
    "f(x) = w^T x + b\n",
    "\n",
    "If f(x) is positive, the input vector is classified as belonging to the positive class, and if it is negative, the input vector is classified as belonging to the negative class.\n",
    "\n",
    "The weights w and bias b are learned from the training data by minimizing the objective function:\n",
    "\n",
    "minimize 1/2 ||w||^2 + C Σ_i ξ_i\n",
    "subject to y_i (w^T x_i + b) ≥ 1 - ξ_i, ξ_i ≥ 0\n",
    "\n",
    "where ||w||^2 is the squared norm of the weight vector, C is a hyperparameter that controls the trade-off between maximizing the margin and minimizing the classification error, y_i is the class label of the i-th training example, and ξ_i is a slack variable that allows for some misclassification of the training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67117a4-925e-479a-80c7-df7603cea9d3",
   "metadata": {},
   "source": [
    "# Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a084cc-3fa1-42e5-bc15-4bb5123ee0f7",
   "metadata": {},
   "source": [
    "The objective function of a linear Support Vector Machine (SVM) is to find the optimal hyperplane that separates the two classes in the training data with maximum margin. The objective function can be expressed as follows:\n",
    "\n",
    "minimize 1/2 ||w||^2 + C Σ_i ξ_i\n",
    "\n",
    "subject to y_i (w^T x_i + b) ≥ 1 - ξ_i, ξ_i ≥ 0\n",
    "\n",
    "where w is a vector of weights, b is a bias term, and x_i and y_i represent the input features and output labels, respectively, of the i-th training example. The term ||w||^2 represents the squared norm of the weight vector and is used to maximize the margin between the decision boundary and the closest points from each class. The constant C is a hyperparameter that controls the trade-off between maximizing the margin and minimizing the classification error.\n",
    "\n",
    "The second part of the objective function is the regularization term that controls the number of misclassifications allowed in the training data. The slack variable ξ_i is introduced to allow for some misclassification of the training examples. The quantity CΣ_i ξ_i represents the penalty for misclassification, and by minimizing this term, the SVM tries to ensure that the misclassification is kept to a minimum while still maximizing the margin.\n",
    "\n",
    "The optimization problem is solved using quadratic programming techniques to obtain the optimal values of w and b that define the decision boundary.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add6208-c127-4ddd-a1e0-34f12ab1b9cd",
   "metadata": {},
   "source": [
    "# Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c4515-995d-4d20-97e3-17ad747d9589",
   "metadata": {},
   "source": [
    "The kernel trick in Support Vector Machines (SVMs) is a technique that allows SVMs to handle nonlinearly separable data without explicitly transforming the input features into a higher-dimensional space.\n",
    "\n",
    "The basic idea of the kernel trick is to introduce a kernel function that computes the dot product of the input feature vectors in a higher-dimensional space, without actually computing the feature vectors themselves. This allows the SVM to implicitly work with the data in the higher-dimensional space, while still operating in the original input space.\n",
    "\n",
    "In other words, the kernel function defines a similarity measure between pairs of input feature vectors, which is used by the SVM to find a decision boundary that maximizes the margin between the classes. The kernel function maps the input features into a higher-dimensional space, where it may be possible to find a linear decision boundary that separates the classes.\n",
    "\n",
    "The most commonly used kernel functions are the radial basis function (RBF) kernel, the polynomial kernel, and the linear kernel. The RBF kernel is often used for its flexibility and ability to capture complex patterns in the data, while the polynomial kernel is used for its simplicity and ability to capture nonlinear relationships in the data. The linear kernel is used when the data is linearly separable.\n",
    "\n",
    "The kernel trick allows SVMs to handle high-dimensional data and nonlinear decision boundaries, without incurring the computational cost of explicitly transforming the input features into a higher-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f711d-6bd0-428c-8ee2-91f604f04d01",
   "metadata": {},
   "source": [
    "# Q4. What is the role of support vectors in SVM Explain with example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e60dfb5-f3ed-4f7f-b493-3cd31dec27f7",
   "metadata": {},
   "source": [
    "Support vectors are the data points that lie closest to the decision boundary or the margin in a Support Vector Machine (SVM). These are the data points that are most critical to the performance of the SVM, as they determine the position and orientation of the decision boundary.\n",
    "\n",
    "The role of support vectors in SVM can be illustrated with the help of an example. Consider a simple two-dimensional dataset with two classes that are not linearly separable in the input space. The SVM tries to find a linear decision boundary that maximizes the margin between the classes. However, since the classes are not linearly separable, the SVM uses a kernel function to map the input data into a higher-dimensional space where it may be possible to find a linear decision boundary.\n",
    "\n",
    "During the training process, the SVM identifies the support vectors that lie closest to the decision boundary or the margin. These support vectors are the data points that are most difficult to classify, as they lie closest to the decision boundary. The SVM uses these support vectors to define the decision boundary and compute the margins.\n",
    "\n",
    "Once the decision boundary is defined, the SVM can be used to classify new input data points. The SVM calculates the distance of each new input data point from the decision boundary and assigns it to the class that is closest to it. The support vectors play a crucial role in this process, as they determine the position and orientation of the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d08d646-ddd3-41c5-a38b-e5222d9f33cb",
   "metadata": {},
   "source": [
    "# Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf18151-22b9-4477-b506-4b6fcda7f2f0",
   "metadata": {},
   "source": [
    "- (1) Hyperplane:\n",
    "\n",
    "In SVM, the hyperplane is the decision boundary that separates the two classes in the feature space. It is a linear classifier that tries to find the best hyperplane that separates the two classes with the maximum margin. The hyperplane is defined by the weight vector w and the bias term b.\n",
    "\n",
    "Example: Consider a 2D dataset with two classes, labeled as red and blue. The following graph shows the hyperplane that separates the two classes using a linear kernel.\n",
    "\n",
    "- (2) Marginal plane:\n",
    "The marginal plane is a plane parallel to the hyperplane that separates the support vectors from the rest of the training data. The distance between the marginal plane and the hyperplane is called the margin. In other words, the marginal plane is the boundary that encloses the support vectors.\n",
    "\n",
    "Example: The following graph shows the hyperplane and the marginal plane for the same 2D dataset. The blue and red points are the support vectors. The distance between the hyperplane and the marginal plane is the margin.\n",
    "\n",
    "\n",
    "- (3) Soft margin:\n",
    "The concept of soft margin is used when the training data is not linearly separable. In soft margin SVM, the margin is allowed to be violated by some training data points, but the violations are penalized. The soft margin SVM uses a slack variable to allow some misclassifications. The amount of misclassification is controlled by a hyperparameter called C.\n",
    "\n",
    "Example: The following graph shows a 2D dataset that is not linearly separable. The soft margin SVM finds the best hyperplane that separates the two classes while allowing some misclassifications. The blue and red points represent the support vectors, while the dotted line represents the margin.\n",
    "\n",
    "- (4) Hard margin:\n",
    "The concept of hard margin is used when the training data is linearly separable. In hard margin SVM, no misclassification is allowed, and the margin is maximized. The hard margin SVM does not use a slack variable and tries to find the best hyperplane that separates the two classes without any misclassifications.\n",
    "\n",
    "Example: The following graph shows a 2D dataset that is linearly separable. The hard margin SVM finds the best hyperplane that separates the two classes without any misclassifications. The blue and red points represent the support vectors, while the dotted line represents the margin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc32a91-a1a2-49b2-83d0-a11cebe09f8d",
   "metadata": {},
   "source": [
    "# Q6. SVM Implementation through Iris dataset.\n",
    "# ~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "# ~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "# ~ Compute the accuracy of the model on the testing setl\n",
    "# ~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "# ~ Try different values of the regularisation parameter C and see how it affects the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d05b9-eb0b-4262-99f2-b083927ad2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier on the training set\n",
    "svm_clf = SVC(kernel='linear', C=1)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of SVM classifier on Iris dataset: {:.2f}%\".format(accuracy*100))\n",
    "\n",
    "# Plot the decision boundaries of the trained model using two of the features\n",
    "# We choose Sepal Length and Sepal Width as the two features to plot\n",
    "X = iris.data[:, :2]  # Only the first two features\n",
    "y = iris.target\n",
    "\n",
    "# Plot the decision boundaries\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('Decision Boundary for Iris Dataset using SVM')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
