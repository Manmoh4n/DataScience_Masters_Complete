{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "264fec27-73b2-44db-a02d-86f34e90cd15",
   "metadata": {},
   "source": [
    "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb1a0ec-3e25-4899-b451-75afe15f1bcc",
   "metadata": {},
   "source": [
    "To find the probability that an employee is a smoker given that they use the health insurance plan, we can use Bayes' theorem.\n",
    "\n",
    "Let's define the events as follows:\n",
    "A = Employee uses the health insurance plan\n",
    "S = Employee is a smoker\n",
    "\n",
    "We are given:\n",
    "P(A) = 0.70 (probability that an employee uses the health insurance plan)\n",
    "P(S|A) = 0.40 (probability that an employee is a smoker given that they use the health insurance plan)\n",
    "\n",
    "We want to calculate P(S|A), the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "\n",
    "According to Bayes' theorem:\n",
    "P(S|A) = (P(A|S) * P(S)) / P(A)\n",
    "\n",
    "We need the probability of an employee using the health insurance plan given that they are a smoker, P(A|S), and the overall probability of an employee being a smoker, P(S).\n",
    "\n",
    "Given the information provided, we don't have direct values for P(A|S) or P(S). Therefore, we cannot calculate the exact probability of an employee being a smoker given that they use the health insurance plan without additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b9e34-5db7-494a-af09-1e550a45b6da",
   "metadata": {},
   "source": [
    "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef26164-c8f1-420c-8bd6-7de2c3c3a897",
   "metadata": {},
   "source": [
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of features they can handle and the underlying probability distributions they assume.\n",
    "\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Bernoulli Naive Bayes is suitable for binary features, where each feature represents the presence or absence of a particular attribute.\n",
    "It assumes that features are independent and follow a Bernoulli distribution, which means each feature can take only two values (0 or 1).\n",
    "It is commonly used for document classification tasks or problems with binary features, such as text sentiment analysis, where the focus is on whether a word is present in a document or not.\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Multinomial Naive Bayes is suitable for discrete features that represent counts or occurrence frequencies.\n",
    "It assumes that features are independent and follow a multinomial distribution, which means each feature can take multiple discrete values.\n",
    "It is commonly used in text classification tasks where features represent word frequencies or occurrence counts in a document.\n",
    "In Multinomial Naive Bayes, feature values are typically non-negative integers, such as the number of times a word appears in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77440c78-e9ff-4ce6-a410-483d3094caa9",
   "metadata": {},
   "source": [
    "# Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6eb170-834c-4f99-a569-271b4a81b85f",
   "metadata": {},
   "source": [
    "\n",
    "Bernoulli Naive Bayes does not handle missing values explicitly in its standard form. It assumes that all features are binary and assumes the presence or absence of each feature. Therefore, if a feature value is missing, it cannot be directly accommodated within the Bernoulli Naive Bayes framework.\n",
    "\n",
    "However, there are a few common approaches to handling missing values in Bernoulli Naive Bayes:\n",
    "\n",
    "Complete case analysis: One simple approach is to remove any instances with missing values from the dataset before applying Bernoulli Naive Bayes. This approach discards instances with missing values, potentially resulting in data loss.\n",
    "\n",
    "Missing value as a separate category: Another option is to treat missing values as a separate category or create a new binary feature to represent the presence or absence of a value. This approach enables the inclusion of missing values within the framework of Bernoulli Naive Bayes but introduces an additional category that represents missingness.\n",
    "\n",
    "Imputation: You can also use imputation techniques to fill in missing values. For Bernoulli Naive Bayes, you may impute missing values with the mode (most frequent value) of the respective feature. This approach allows you to retain instances with missing values while maintaining binary feature values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed85ad-c61b-4953-9798-fdbe477b7221",
   "metadata": {},
   "source": [
    "# Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00465fab-68ed-4b7b-ac8f-4bb01c7d3cf5",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. While the name \"Naive Bayes\" suggests that it is primarily used for binary classification, Gaussian Naive Bayes can be extended to handle multi-class problems.\n",
    "\n",
    "In Gaussian Naive Bayes, it is assumed that the features in each class follow a Gaussian (normal) distribution. When applied to multi-class classification, the model estimates the parameters of the Gaussian distribution (mean and variance) for each feature in each class.\n",
    "\n",
    "During the classification process, the model calculates the likelihood of each class for a given instance using the probability density function (PDF) of the Gaussian distribution. The class with the highest likelihood is then assigned as the predicted class for that instance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
